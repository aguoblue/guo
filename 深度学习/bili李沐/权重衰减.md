

  
在 PyTorch 中，权重衰减（Weight Decay）通常是作为优化器的一个参数来实现的。权重衰减是正则化技术的一种，用于防止模型过拟合。它的工作原理类似于L2正则化，通过在损失函数中添加一个与模型权重的平方成比例的项来实现。

权重衰减在优化器中实现，例如在 `torch.optim.SGD` 或 `torch.optim.Adam` 等优化器的构造函数中。当创建一个优化器实例时，可以通过 `weight_decay` 参数来设置权重衰减的系数。

下面是一个使用 `torch.optim.SGD` 优化器并设置权重衰减的示例：

```python
import torch.optim as optim

# 假设 net 是一个 PyTorch 模型
optimizer = optim.SGD(net.parameters(), lr=0.01, weight_decay=0.001)

```

  
在这个例子中，`weight_decay=0.001` 表明权重衰减系数为0.001。这个值决定了权重衰减的强度：值越大，正则化效果越强，权重趋向于更小的值，这有助于防止过拟合；值越小，正则化效果越弱。


在 PyTorch 中，当使用优化器如 `torch.optim.SGD` 或 `torch.optim.Adam` 等时，`weight_decay` 参数的默认值通常是 `0`。这意味着，除非你明确地设置了一个非零的 `weight_decay` 值，否则默认情况下不会有权重衰减效果。



使用有噪音的数据等价于Tikhonov正则
丢弃法：在层之间加入噪音



深度学习中常见的一种模式：在使用交叉熵损失函数时，模型的输出层不需要显式包含softmax激活函数。这是因为许多深度学习框架（包括PyTorch）提供了一个结合了softmax和交叉熵计算的损失函数，例如PyTorch中的 `nn.CrossEntropyLoss`。这种组合在数值上更稳定，也更高效。

1. **数值稳定性**：将softmax和交叉熵损失的计算结合起来可以提高数值稳定性。独立计算softmax和交叉熵可能导致数值不稳定，因为softmax会产生非常小的概率值，而对这些值取对数（在交叉熵中进行）可能导致数值问题。
    
2. **效率**：合并这两个操作也更加高效，因为它减少了计算步骤和中间数据的存储