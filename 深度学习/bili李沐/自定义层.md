


自定义层、块
	不带参数
	带参数


```python
class MLP(nn.Module):
    # 用模型参数声明层。这里，我们声明两个全连接的层
    def __init__(self):
        # 调用MLP的父类Module的构造函数来执行必要的初始化。
        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）
        super().__init__()
        self.hidden = nn.Linear(20, 256)  # 隐藏层
        self.out = nn.Linear(256, 10)  # 输出层

    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出
    def forward(self, X):
        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。
        return self.out(F.relu(self.hidden(X)))
```


在PyTorch中，调用 `net(X)` 实际上是调用 `net.forward(X)` 的一种简写形式，但是有一些重要的区别和背后的原因：

1. **`__call__` 方法：** 当您对一个 PyTorch 模型实例（例如 `net`）执行 `net(X)` 时，实际上是在调用模型的 `__call__` 方法。这个方法被定义在 `nn.Module` 的基类中，它不仅调用 `forward` 方法，还处理了其他一些操作，比如设置模块为训练/评估模式、应用钩子函数等。
    
2. **额外功能：** 使用 `__call__` 方法允许PyTorch在调用 `forward` 方法之前和之后执行一些额外的操作。例如，它可以在训练和评估模式之间切换，应用在模型上注册的钩子，以及进行自动梯度计算的设置。
    
3. **推荐使用 `net(X)`：** 正因为这些额外的功能，PyTorch推荐使用 `net(X)` 而不是直接调用 `forward` 方法。直接调用 `forward` 方法可能会跳过一些重要的框架级别的处理步骤，特别是当涉及到模型的钩子和其他高级功能时。
    
4. **保持代码的清晰和一致性：** 使用 `net(X)` 使得代码更加清晰和一致，同时也更符合Python对象调用的常规习惯。
    

总之，虽然 `net.forward(X)` 和 `net(X)` 在很多情况下看似相同，但实际上 `net(X)` 是更标准和安全的使用方式，因为它确保所有PyTorch的内部机制都能正确运作。


顺序块


```python
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员
            # 变量_modules中。_module的类型是OrderedDict
            self._modules[str(idx)] = module

    def forward(self, X):
        # OrderedDict保证了按照成员添加的顺序遍历它们
        for block in self._modules.values():
            X = block(X)
        return X

net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
net(X)
```


参数访问

参数初始化
	内置初始化
	自定义初始化
	参数共享


读写

保存、读取张量
加载和保存模型参数





